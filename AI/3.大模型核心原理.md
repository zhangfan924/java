## 🧩 Token 与 Embedding（嵌入向量）

1. **Token 是 NLP 的最小单元**
   - 类似编程语言里的 **关键字/变量名/符号**。
   - 模型不直接吃原始文本，而是先把文字转成 **Token IDs**（整数）。
2. **为什么要转成浮点向量？**
   - 计算机擅长处理数字，但 **ID=3 并不能体现“你”的语义**。
   - 所以要把 ID → 高维浮点向量（Embedding），这样才有“语义空间”的几何关系。
3. **浮点数的意义**
   - 刚开始：这些数是随机的（高斯分布）。
   - 训练后：通过 **梯度下降 + 反向传播**，逐步调整，直到向量能体现语义：
     - “猫”和“狗” → 向量距离近
     - “苹果”和“宇宙” → 向量距离远

📌 类比数据库：

- Token ID = 主键ID
- Embedding = 多维索引（类似 R-Tree/向量索引），能快速找到“相似”的语义。

------

## ⚡ Transformer 与自注意力机制

Transformer 之所以牛，是因为它用 **Self-Attention（自注意力）** 替代了传统 RNN/CNN 的序列处理。

1. **注意力机制本质**：
   - 模型在处理某个 Token 时，会“权衡”该 Token 应该多关注其他哪些 Token。
   - 例如：
      `"The cat ate the fish"`
     - 处理 `"ate"` 时，模型会高度关注 `"cat"` 和 `"fish"`，而不是 `"the"`。
2. **数学公式（点到为止）**：
   - 每个 Token 会被映射成：
     - Query（Q）= 我要问谁？
     - Key（K）= 我是谁？
     - Value（V）= 我能提供什么信息？
   - 注意力分数 = `Q · K / sqrt(d)` → Softmax → 权重分布
   - 最终输出 = ∑ (权重 × V)

📌 类比 SQL：

- `Q` = 查询条件（where）
- `K` = 索引字段
- `V` = 返回的结果列
- Self-Attention = 动态优化的多表 join

------

## 💻 你的 Java 示例对比

1. **Token → ID → 向量**

   ```
   String text = "你好世界！人工智能";
   List<Integer> tokenIds = tokenize(text);
   float[][] embeddings = tokensToEmbeddings(tokenIds);
   ```

   ✅ 演示了从文本到语义向量的过程。

2. **自注意力机制**

   Transformer 中的 **注意力机制（Attention Mechanism）** 是模型的核心组件，它通过动态计算输入序列中各个部分的重要性权重，实现对上下文信息的灵活捕捉。其核心思想是：**让模型在处理某个 Token 时，能够自主决定应该关注输入序列中的哪些其他 Token**

   以翻译句子为例：

   * 输入序列：`"The cat ate the fish"`
   * 当模型处理 `"ate"` 时，注意力机制会让它：
     * 高度关注 `"cat"`（主语）和 `"fish"`（宾语），
     * 忽略无关词（如 `"the"`）

   ```
   String currentToken = "ate";
   Map<String, Float> attentionWeights = calculateAttention(currentToken);
   ```

   ✅ 演示了 `"ate"` 如何计算注意力，最终“关注” `"cat"` 和 `"fish"`。

这两个代码片段正好把 **Embedding + Self-Attention** 串起来了，形成 **Transformer 的最小工作流**。

